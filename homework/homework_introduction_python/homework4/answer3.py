"""
3\. ä½¿ç”¨æ ‡å‡†åº“å†…ç½®æ¨¡å—å†™ä¸€ä¸ªæœ€ç®€å•çš„MAPREDUCEä¾‹å­
åˆ†æä¸€ä¸‹é‡‘åº¸å°è¯´ä¸­ï¼Œé‡‘åº¸æœ€å–œæ¬¢ç”¨çš„çŸ­å¥æ˜¯é‚£äº›ï¼Ÿ

è¦æ±‚ï¼š

é‡‘åº¸å°è¯´å¯ä»¥ç½‘ä¸Šå»æ‰¾
ä½¿ç”¨åœç”¨è¯ï¼Œ https://github.com/chdd/weibo/blob/master/stopwords/ä¸­æ–‡åœç”¨è¯åº“.txt
```
â¯ python 3.py
ForkPoolWorker-1 reading novels/3.txt
ForkPoolWorker-3 reading novels/2.txt
ForkPoolWorker-2 reading novels/1.txt


é‡‘åº¸æœ€çˆ±è¯´ï¼šğŸ˜‰

è¿‡äº†ä¸€ä¼š  : count:72
æ‹çš„ä¸€å£°  : count:69
ç«™èµ·èº«æ¥  : count:59
ç °çš„ä¸€å£°  : count:52
éŸ¦å°å®å¤§å–œ : count:50
å¿ƒä¸­å¤§å–œ  : count:46
å¦åˆ™çš„è¯  : count:44
æ˜¯äº†    : count:41
è¿‡ä¸å¤šæ—¶  : count:40
çªç„¶ä¹‹é—´  : count:38
```

æç¤ºï¼šcodecs.openå¤„ç†æ–‡ä»¶ç¼–ç ã€multiprocessing.Poolçš„mapæ–¹æ³•ã€ä¹‹å‰çš„å»¶ä¼¸é˜…è¯»é“¾æ¥
"""

import collections
import itertools
import multiprocessing
import operator


class SimpleMapReduce:
    def __init__(self, map_func, reduce_func, num_workers=None):
        self.map_func = map_func
        self.reduce_func = reduce_func
        self.pool = multiprocessing.Pool(num_workers)

    def partition(self, mapped_values):
        partitioned_data = collections.defaultdict(list)
        for key, value in mapped_values:
            partitioned_data[key].append(value)
        return partitioned_data.items()

    def __call__(self, inputs, chunksize=1):
        map_responses = self.pool.map(self.map_func, inputs, chunksize=chunksize)
        partitioned_data = self.partition(itertools.chain(*map_responses))
        reduced_values = self.pool.map(self.reduce_func, partitioned_data)
        return reduced_values


def file_to_words(filename):
    print("{} reading {}".format(multiprocessing.current_process().name, filename))

    output = []
    words = []
    with open("chinese_words.txt", "rb") as d:
        for line in d:
            words.append(line.decode("gbk").strip())
    with open(filename, "rb") as f:
        for line in f:
            for word in words:
                if word in line.decode():
                    output.append((word, 1))
    return output


def count_words(item):
    word, occurences = item
    return (word, sum(occurences))


input_files = ["xiaoshuo1.txt", "xiaoshuo2.txt", "xiaoshuo3.txt"]
mapper = SimpleMapReduce(file_to_words, count_words)
word_counts = mapper(input_files)
word_counts.sort(key=operator.itemgetter(1))

print("\nTOP 20 WORDS BY FREQUENCY\n")
top20 = word_counts[-20:]
longest = max(len(word) for word, count in top20)
for word, count in top20:
    print("{word:<{len}}: {count:5}".format(len=longest + 1, word=word, count=count))
